{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCn5gK-8Qhtz"
   },
   "source": [
    "#  Instruction\n",
    "\n",
    "In this notebook, we will learn how to implement Vanilla Policy Gradient (VPG) using Tensorflow for the [Pendulum environment in OpenAI gym](https://gymnasium.farama.org/environments/classic_control/pendulum/). You are given a basic skeleton but you need to complete the code where appropriate to solve the Pendulum problem.\n",
    "\n",
    "You are free to tweak the code at any part. You are also free to tweak the hyper-parameters to improve the performance of the agent. At the end you have to evaluate the performance of the agent on 100 independent episodes of the environment and print out the average testing performance.\n",
    "\n",
    "Make sure that your final submission is a notebook that can be run from beginning to end, and that you print out the performance of the agent at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7-Vo-FNPRX9V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# !{os.sys.executable} -m pip install gymnasium\n",
    "# !{os.sys.executable} -m pip install Pillow\n",
    "# !{os.sys.executable} -m pip install ipython\n",
    "# !{os.sys.executable} -m pip install pygame\n",
    "# !{os.sys.executable} -m pip install tensorflow_probability\n",
    "# !{os.sys.executable} -m pip install tensorflow\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rV8-5DGUv0QY"
   },
   "outputs": [],
   "source": [
    "# Environment\n",
    "envname = 'Pendulum-v1'  # Environment ID for gym\n",
    "env=gym.make(envname, render_mode=\"rgb_array\")\n",
    "\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GujIeXW2RulT"
   },
   "source": [
    "## Vanilla Policy Gradient (VPG)\n",
    "\n",
    "Having recently implemented Deep Q-Networks (DQN) in HW4, we've seen firsthand how neural networks can approximate the action-value function, facilitating Q-learning in environments with complex state spaces. This time, we'll shift our focus towards directly optimizing policies through Policy Gradient, an alternative approach that offers unique advantages, especially in scenarios involving continuous action spaces or the need for stochastic policies.\n",
    "\n",
    "Policy Gradient methods offer a direct approach to policy optimization. Instead of approximating the action-value function and deriving a policy as in DQN, Policy Gradient methods directly parametrize the policy $\\pi_\\theta\\left(a \\mid s\\right)$ and optimize over the parameter vector $\\theta$. This direct optimization aims to find the policy that maximizes the expected return from any given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9BlBZLeSh2f"
   },
   "source": [
    "To employ VPG, we train a neural network to optimize parameters $\\theta$ for the policy $\\pi_\\theta(a \\mid s)$. Utilizing sequences of states, actions, and rewards from trajectories $\\left(s_t, a_t, r_{t}\\right)$, we utilize stochastic gradient ascent to adjust $\\theta$ towards enhancing the policy's expected return. This direct optimization of $\\theta$ using observed environment interactions advances the policy, focusing on increasing cumulative rewards. To address the high variance in gradient estimates, we introduce a baseline $b\\left(s_t\\right)$, representing the expected return under the current state $s_t$, and adjust the policy gradient by subtracting this baseline from the returns. This adjustment, performed during policy update steps, enables more stable and efficient learning. Hence, VPG-with-baseline can be viewed as an actor-critic algorithm, where the policy $\\pi_\\theta(a \\mid s)$ functions as the actor, determining the action for a given state, while the baseline $b\\left(s_t\\right)$ can be seen as the critic, refining the policy's performance.\n",
    "\n",
    "References:\n",
    "- Policy Gradient Methods for Reinforcement Learning with Function Approximation, Sutton et al. 2000\n",
    "- High Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRR4MH92hs4D"
   },
   "source": [
    "### Build Actor and Critic Models\n",
    "In this setup, we deploy an actor-critic architecture for handling environments with continuous action spaces. A Gaussian policy is adopted for action probability distribution at each state. The actor part of the model utilizes a four-layer neural network to predict the mean of this distribution for any input state. Moreover, we treat the logarithm of the standard deviation parameter as a state-independent parameter, which allows for more nuanced control over action variability. For the critic part (which computes baseline $b(s_t)$), the most common choice of the baseline is the on-policy value function $V^\\pi\\left(s_t\\right)$, which is approximated using a four-layer fully-connected neural network. Although the provided code is pre-configured for this task, it allows for adjustments to further optimize the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eaD9EkXmhcRl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General Instructions\n",
    "1. The default hyperparameters are set to solve the Pendulum-v1 problem in a continuous environment.\n",
    "2. While there is no need to modify the rest of the code for this task, feel free to do so if needed.\n",
    "\"\"\"\n",
    "\n",
    "def actor_creator(state_dim, action_dim, log_std_initial_value=0.0):\n",
    "    \"\"\"\n",
    "    Creates an actor model suitable for continuous action spaces in reinforcement learning.\n",
    "    Outputs actions based on a mean policy and has a learnable logarithm of the standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - state_dim (int): Dimension of the input state.\n",
    "    - action_dim (int): Dimension of the output action.\n",
    "    - log_std_initial_value (float): Initial value for logarithm of the standard deviation for the action distribution\n",
    "\n",
    "    Returns:\n",
    "    - A Keras model that outputs the mean for the action distribution and has a trainable logarithm of the standard deviation\n",
    "    \"\"\"\n",
    "    # Define the input layer for states\n",
    "    state_input = layers.Input(shape=(state_dim,))\n",
    "\n",
    "    # Define two hidden layers with tanh activation\n",
    "    hidden1 = layers.Dense(64, activation='tanh')(state_input)\n",
    "    hidden2 = layers.Dense(64, activation='tanh')(hidden1)\n",
    "\n",
    "    # Define the output layer for the action distribution's mean\n",
    "    mu_output = layers.Dense(action_dim, activation=None)(hidden2)\n",
    "\n",
    "    # Initialize a trainable log standard deviation, independent of the state\n",
    "    log_std = tf.Variable(initial_value=np.full((action_dim,), log_std_initial_value), dtype=tf.float32, trainable=True)\n",
    "\n",
    "    # Output log standard deviation directly, using a Lambda layer\n",
    "    log_std_output = layers.Lambda(lambda x: tf.expand_dims(log_std, axis=0))(state_input)\n",
    "\n",
    "    # Construct and return the actor model\n",
    "    model = models.Model(inputs=state_input, outputs=[mu_output, log_std_output])\n",
    "    return model\n",
    "\n",
    "def critic_creator(state_dim):\n",
    "    \"\"\"\n",
    "    Creates a critic model for estimating state values.\n",
    "\n",
    "    Parameters:\n",
    "    - state_dim (int): Dimension of the input state.\n",
    "\n",
    "    Returns:\n",
    "    - A Keras model that outputs the value of given states.\n",
    "    \"\"\"\n",
    "    # Define the input layer for states\n",
    "    state_input = layers.Input(shape=(state_dim,))\n",
    "\n",
    "    # Define hidden layers and the output layer for state value estimation\n",
    "    hidden1 = layers.Dense(64, activation='tanh')(state_input)\n",
    "    hidden2 = layers.Dense(64, activation='tanh')(hidden1)\n",
    "    value_output = layers.Dense(1, activation=None)(hidden2)\n",
    "\n",
    "    # Construct and return the critic model\n",
    "    model = models.Model(inputs=state_input, outputs=value_output)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7bHqxD-lwyr"
   },
   "source": [
    "### Sample Trajectory\n",
    "Implement sample_traj function to sample trajectories from the environment.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aawoMYcIlxNx"
   },
   "outputs": [],
   "source": [
    "def sample_traj(batch_size=2000, seed=None):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to samples trajectories from the environment using the provided actor model.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int): The number of states visited.\n",
    "    - seed (int, optional): Seed for the environment's random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - States, actions, rewards, not_dones, and average episodic reward.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, not_dones = [], [], [], []\n",
    "    curr_reward_list = []\n",
    "\n",
    "    # Continue sampling until reaching the specified batch size\n",
    "    while len(states) < batch_size:\n",
    "        # Reset the environment at the start or after each episode ends\n",
    "        state = env.reset(seed=seed)[0]\n",
    "        curr_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Sample actions from the actor and step through the environment until the episode ends\n",
    "        while not done:\n",
    "            # Prepare the current state for the actor model\n",
    "            state_tensor = np.expand_dims(np.array(state, dtype=np.float32), axis=0)\n",
    "\n",
    "            # Get the mean and log standard deviation of action distribution\n",
    "            mean, log_std = actor(state_tensor)\n",
    "\n",
    "            # Calculate the standard deviation\n",
    "            std = tf.math.exp(log_std)\n",
    "\n",
    "            # Sample an action from the Gaussian distribution\n",
    "            # action = ...\n",
    "\n",
    "            # Execute the action in the environment to get the next state and reward\n",
    "            next_state, reward, terminated, truncated,_ = env.step(action)\n",
    "\n",
    "            # Check if the episode has ended\n",
    "            # done = ...\n",
    "\n",
    "            # Store the current state, action, reward, and continuation flag\n",
    "            # states...\n",
    "            # actions...\n",
    "            # rewards...\n",
    "            # not_dones...\n",
    "\n",
    "            # Prepare for the next step\n",
    "            state = next_state\n",
    "            curr_reward += reward\n",
    "\n",
    "            # Exit the loop if the episode has ended\n",
    "            if done:\n",
    "              break\n",
    "\n",
    "        # Keep track of the cumulative reward for this episode\n",
    "        curr_reward_list.append(curr_reward)\n",
    "\n",
    "    # Return the sampled data and the average reward per episode\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(not_dones), np.mean(curr_reward_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHaI6w4BSc3Q"
   },
   "source": [
    "### Training Function\n",
    "As we saw in the class, the objective in Policy Gradient is to find policy parameters $\\theta$ that maximize the policy value (expected return), which is formalized as(for $\\gamma=1$):\n",
    "\\begin{equation}\n",
    "\\arg \\max _\\theta V(\\theta) = \\arg \\max _\\theta \\sum_\\tau P(\\tau ; \\theta) R(\\tau),\n",
    "\\end{equation}\n",
    "where $\\tau=\\left(s_0, a_0, r_0, \\ldots, s_{T-1}, a_{T-1}, r_{T-1}\\right)$ is a state-action trajectory,\n",
    "$P(\\tau ; \\theta)$ is probability of observing trajectory $\\tau$ when using policy $\\pi_\\theta$ starting from state $s_0$ and\n",
    "$R(\\tau)=\\sum_t R\\left(s_t, a_t\\right)$ is the sum of rewards in trajectory $\\tau$.\n",
    "\n",
    "In the lecture, the gradient of the expected return, approximated with empirical estimates from $m$ trajectories under policy $\\pi_\\theta$, and leveraging temporal structure, is given by\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] \\approx \\frac{1}{m} \\sum_{i=1}^m \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta\\left(a^{(i)}_t \\mid s^{(i)}_t\\right) G_t^{(i)},\n",
    "\\end{equation}\n",
    "where $G_t^{(i)}$ is the reward-to-go (the sum of rewards after a point in a trajectory) and $G_t^{(i)}=\\sum_{t^{\\prime}=t}^{T-1} r_{t^{\\prime}}^{(i)}$.\n",
    "\n",
    "To mitigate the variance of the gradient estimates, we introduce a critic network, which fits a baseline function $b\\left(s^{(i)}_t\\right)$ to the reward-to-go $G_t^{(i)}$, by minimizing the loss function:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}=\\frac{1}{mT} \\sum_{i=1}^m \\sum_{t=0}^{T-1} \\left\\|b\\left(s^{(i)}_t\\right)-G_t^{(i)}\\right\\|^2.\n",
    "\\end{equation}\n",
    "\n",
    "For a paritcular trajectory $i$, the advantage estimate $\\hat{A}_t^{(i)}$ is formally defined as the difference of $G_t^{(i)}$ from the baseline value of $s_t$ :\n",
    "\\begin{equation}\n",
    "\\hat{A}_t^{(i)}=G^{(i)}_t-b\\left(s^{(i)}_t\\right).\n",
    "\\end{equation}\n",
    "Incorporating the advantage estimate, the gradient estimate based on $m$ trajectories is refined to:\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)]\\approx \\frac{1}{m} \\sum_{i=1}^m \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta\\left(a^{(i)}_t \\mid s^{(i)}_t\\right) \\hat{A}_t^{(i)}.\n",
    "\\end{equation}\n",
    "This gradient is then utilized in stochastic gradient ascent, iteratively adjusting $\\theta$ to improve the policy based on the accumulated experiences from the environment.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wyqgh_wzgjEj"
   },
   "outputs": [],
   "source": [
    " def train(states, actions, rewards, n_dones):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to update the actor and the critic model based on the collected batch of experience.\n",
    "\n",
    "    Parameters:\n",
    "    - states: Observed states from the environment.\n",
    "    - actions: Actions taken by the actor.\n",
    "    - rewards: Rewards received for taking actions.\n",
    "    - n_dones: Indicates whether the episode continues (1) or ends (0).\n",
    "    \"\"\"\n",
    "    # Convert to tensor\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.float32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    n_dones = np.array(n_dones, dtype=np.float32)\n",
    "\n",
    "    '''\n",
    "    TODO:\n",
    "    1. Compute the reward-to-go G_t using discount_rewards function\n",
    "    2. Compute values of states, this will be used as the baseline\n",
    "    3. Update the critic model to predict the reward-to-go G_t for each state\n",
    "    4. Compute log probabilities and advantages\n",
    "    5. Compute the loss of the actor model based on policy gradient estimate and update the actor\n",
    "    '''\n",
    "    # Calculate discounted rewards (num_traj is the number of trajectories)\n",
    "    G_t, num_traj = discount_rewards(rewards, n_dones)\n",
    "\n",
    "    # Update the critic model\n",
    "    with tf.GradientTape() as tape:\n",
    "        # critics = critic(states, training=True)\n",
    "        # critic_loss = ...\n",
    "\n",
    "    critic_grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "    critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "\n",
    "    # Update the actor model\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute log probabilities\n",
    "        # means, log_stds = actor(states)\n",
    "        # stds = tf.exp(log_stds)\n",
    "        # neg_log_prob = 0.5 * tf.reduce_sum(tf.square((actions - means) / (stds + 1e-8)), axis=1)\n",
    "        # neg_log_prob += 0.5 * np.log(2.0 * np.pi)\n",
    "        # neg_log_prob += tf.reduce_sum(log_stds, axis=1)\n",
    "\n",
    "        # Compute and normalize the advantages tensor\n",
    "        # advantages = ...\n",
    "\n",
    "        # Compute the loss based on policy gradient estimate\n",
    "        # actor_loss = tf.reduce_sum(...)/num_traj\n",
    "\n",
    "    actor_grads = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "\n",
    "def discount_rewards(reward_buffer, n_dones):\n",
    "    \"\"\"\n",
    "    TODO: Complete this function to compute the reward-to-go G_t. Note that reward_buffer may include rewards of several trajectories and n_dones can be used to Indicate whether one trajectory ends.\n",
    "\n",
    "    Parameters:\n",
    "    - reward_buffer: The rewards to be processed.\n",
    "    - n_dones: Indicates whether the episode continues (1) or ends (0).\n",
    "\n",
    "    Returns:\n",
    "    - G_t (reward-to-go ), num_traj (the number of trajectories)\n",
    "    \"\"\"\n",
    "    G_t = np.zeros_like(reward_buffer, dtype=float)\n",
    "    running_add = 0\n",
    "    num_traj = 0\n",
    "    for t in reversed(range(len(reward_buffer))):\n",
    "        # Reset the accumulator and count the number of trajectories at the end of each episode\n",
    "        # if n_dones[t] == 0:\n",
    "        #   running_add ...\n",
    "        #   num_traj ...\n",
    "        # running_add = ...\n",
    "        # G_t[t] = ...\n",
    "    return G_t, num_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JA7a0AXiVO7"
   },
   "source": [
    "### Guidelines for Implementing and Optimizing a VPG Agent\n",
    "- **Network Updates**: During each training episode, we collect multiple trajectories and update networks. Each trajectory has 200 state-action-reward samples. Experiment with the batch size (the number of samples collected in one episode) to find a balance between sample efficiency and learning stability.\n",
    "\n",
    "- **Training Scale**: Typically, achieving satisfactory training results requires around 800 to 1200 episodes, with each episode handling a batch size of 5000 (equivalent to 25 trajectories). Note that this range can vary due to the randomness inherent in the Pendulum environment.\n",
    "\n",
    "- **Parameter and Model Architecture Tuning**: Experiment with the parameters and model architecture. Consider implementing a schedule for adjusting parameters such as the learning rate.\n",
    "\n",
    "- **Efficiency**: While achieving high performance with minimal episodes is ideal, focus on implementing VPG effectively. Efficiency improvements are encouraged but not required for grading.\n",
    "\n",
    "- **Debugging Tips**: Monitor the training process by logging the average reward per episode and other metrics. These indicators can help identify training stability and convergence.\n",
    "\n",
    "\n",
    "- **Visualization**: It's important to visualize the agent's performance over time by plotting the running reward during training.\n",
    "\n",
    "- **Objective**: Implement a VPG agent. Your goal is to tweak the neural network and training parameters to achieve a high reward, aiming to consistently achieve rewards above -200. Even so, the grading criteria for rewards are relatively flexible.  \n",
    "\n",
    "*Optional:* Explore advanced techniques for further improvement, such as Advantage Actor-Critic (A2C or A3C). Use generalized advantage estimates (TD with a proper look ahead) instead of using the returns, to improve efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u11sKN5uhUXG"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Implement the training loop to sample trajectories and update the actor and critic models accordingly.\n",
    "\"\"\"\n",
    "# Parameters for training\n",
    "GAMMA = 0.99  # Discount factor for expected discounted sum of rewards\n",
    "last_n_reward = 100  # Number of episodes to consider for calculating running reward\n",
    "\n",
    "# Feel free to change parameters below\n",
    "TRAIN_EPISODES = 1500  # Total number of episodes for training\n",
    "actor_lr = 3e-3  # 4e-3 Learning rate for the actor optimizer\n",
    "critic_lr = 3e-3  # 2e-3 Learning rate for the critic optimizer\n",
    "batch_size = 5000  # Number of steps per batch\n",
    "\n",
    "# Initialization\n",
    "actor = actor_creator(obssize, actsize)  # Create the actor model\n",
    "critic = critic_creator(obssize)  # Create the critic model\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)  # Actor optimizer\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)  # Critic optimizer\n",
    "episode_reward_history = []  # Stores the reward of each episode\n",
    "running_rewards = []  # Stores the running rewards\n",
    "initial_time = time.time()  # Start time for measuring training duration\n",
    "\n",
    "# Training loop\n",
    "for episode in range(TRAIN_EPISODES):\n",
    "    start_time = time.time()  # Start time of the current episode\n",
    "\n",
    "    # Sample trajectories using the current policy\n",
    "    # ... = sample_traj(...)\n",
    "\n",
    "    # Update the actor and critic models using the sampled trajectories\n",
    "    # train(...)\n",
    "\n",
    "    # Save the reward\n",
    "    episode_reward_history.append(episode_reward)\n",
    "\n",
    "    # Keep only the last n rewards\n",
    "    if len(episode_reward_history) > last_n_reward:\n",
    "        del episode_reward_history[0]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    running_rewards.append(running_reward)\n",
    "\n",
    "    # Track the running time\n",
    "    # end_time = time.time()\n",
    "    # episode_runtime = end_time - start_time\n",
    "    # total_runtime = end_time - initial_time\n",
    "\n",
    "    # Print training information\n",
    "    print(f\"Episode: {episode + 1}, Episode Reward: {episode_reward}, Running Reward: {running_reward}, Runtime: {episode_runtime} seconds, Total Runtime: {total_runtime} seconds\")\n",
    "\n",
    "    # Save the actor model if the condition is met\n",
    "    # if episode_reward >= ...:\n",
    "        consistency_count += 1\n",
    "        # if consistency_count == ...:\n",
    "            # actor.save_weights(...)\n",
    "            print(\"Model saved.\")\n",
    "            break\n",
    "    else:\n",
    "        consistency_count = 0\n",
    "\n",
    "# Plotting the running rewards to visualize training progress\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uguKEECCt-8M"
   },
   "source": [
    "### Testing\n",
    "Evaluate the performance of the agent on 100 episodes on the environment and print out the average testing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSY34gwLpgYZ"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE Here\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1aCi4D6WLSJVU2gP9qPxzhIR3pHHGyzem",
     "timestamp": 1712955139312
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
