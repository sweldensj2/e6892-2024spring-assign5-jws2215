{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCn5gK-8Qhtz"
   },
   "source": [
    "#  Instruction\n",
    "\n",
    "In this notebook, we will learn how to implement Vanilla Policy Gradient (VPG) using Tensorflow for the [Pendulum environment in OpenAI gym](https://gymnasium.farama.org/environments/classic_control/pendulum/). You are given a basic skeleton but you need to complete the code where appropriate to solve the Pendulum problem.\n",
    "\n",
    "You are free to tweak the code at any part. You are also free to tweak the hyper-parameters to improve the performance of the agent. At the end you have to evaluate the performance of the agent on 100 independent episodes of the environment and print out the average testing performance.\n",
    "\n",
    "Make sure that your final submission is a notebook that can be run from beginning to end, and that you print out the performance of the agent at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7-Vo-FNPRX9V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# !{os.sys.executable} -m pip install gymnasium\n",
    "# !{os.sys.executable} -m pip install Pillow\n",
    "# !{os.sys.executable} -m pip install ipython\n",
    "# !{os.sys.executable} -m pip install pygame\n",
    "# !{os.sys.executable} -m pip install tensorflow_probability\n",
    "# !{os.sys.executable} -m pip install tensorflow\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rV8-5DGUv0QY"
   },
   "outputs": [],
   "source": [
    "# Environment\n",
    "envname = 'Pendulum-v1'  # Environment ID for gym\n",
    "env=gym.make(envname, render_mode=\"rgb_array\")\n",
    "\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GujIeXW2RulT"
   },
   "source": [
    "## Vanilla Policy Gradient (VPG)\n",
    "\n",
    "Having recently implemented Deep Q-Networks (DQN) in HW4, we've seen firsthand how neural networks can approximate the action-value function, facilitating Q-learning in environments with complex state spaces. This time, we'll shift our focus towards directly optimizing policies through Policy Gradient, an alternative approach that offers unique advantages, especially in scenarios involving continuous action spaces or the need for stochastic policies.\n",
    "\n",
    "Policy Gradient methods offer a direct approach to policy optimization. Instead of approximating the action-value function and deriving a policy as in DQN, Policy Gradient methods directly parametrize the policy $\\pi_\\theta\\left(a \\mid s\\right)$ and optimize over the parameter vector $\\theta$. This direct optimization aims to find the policy that maximizes the expected return from any given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9BlBZLeSh2f"
   },
   "source": [
    "To employ VPG, we train a neural network to optimize parameters $\\theta$ for the policy $\\pi_\\theta(a \\mid s)$. Utilizing sequences of states, actions, and rewards from trajectories $\\left(s_t, a_t, r_{t}\\right)$, we utilize stochastic gradient ascent to adjust $\\theta$ towards enhancing the policy's expected return. This direct optimization of $\\theta$ using observed environment interactions advances the policy, focusing on increasing cumulative rewards. To address the high variance in gradient estimates, we introduce a baseline $b\\left(s_t\\right)$, representing the expected return under the current state $s_t$, and adjust the policy gradient by subtracting this baseline from the returns. This adjustment, performed during policy update steps, enables more stable and efficient learning. Hence, VPG-with-baseline can be viewed as an actor-critic algorithm, where the policy $\\pi_\\theta(a \\mid s)$ functions as the actor, determining the action for a given state, while the baseline $b\\left(s_t\\right)$ can be seen as the critic, refining the policy's performance.\n",
    "\n",
    "References:\n",
    "- Policy Gradient Methods for Reinforcement Learning with Function Approximation, Sutton et al. 2000\n",
    "- High Dimensional Continuous Control Using Generalized Advantage Estimation, Schulman et al. 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRR4MH92hs4D"
   },
   "source": [
    "### Build Actor and Critic Models\n",
    "In this setup, we deploy an actor-critic architecture for handling environments with continuous action spaces. A Gaussian policy is adopted for action probability distribution at each state. The actor part of the model utilizes a four-layer neural network to predict the mean of this distribution for any input state. Moreover, we treat the logarithm of the standard deviation parameter as a state-independent parameter, which allows for more nuanced control over action variability. For the critic part (which computes baseline $b(s_t)$), the most common choice of the baseline is the on-policy value function $V^\\pi\\left(s_t\\right)$, which is approximated using a four-layer fully-connected neural network. Although the provided code is pre-configured for this task, it allows for adjustments to further optimize the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eaD9EkXmhcRl"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General Instructions\n",
    "1. The default hyperparameters are set to solve the Pendulum-v1 problem in a continuous environment.\n",
    "2. While there is no need to modify the rest of the code for this task, feel free to do so if needed.\n",
    "\"\"\"\n",
    "\n",
    "def actor_creator(state_dim, action_dim, log_std_initial_value=0.0):\n",
    "    \"\"\"\n",
    "    Creates an actor model suitable for continuous action spaces in reinforcement learning.\n",
    "    Outputs actions based on a mean policy and has a learnable logarithm of the standard deviation.\n",
    "\n",
    "    Parameters:\n",
    "    - state_dim (int): Dimension of the input state.\n",
    "    - action_dim (int): Dimension of the output action.\n",
    "    - log_std_initial_value (float): Initial value for logarithm of the standard deviation for the action distribution\n",
    "\n",
    "    Returns:\n",
    "    - A Keras model that outputs the mean for the action distribution and has a trainable logarithm of the standard deviation\n",
    "    \"\"\"\n",
    "    # Define the input layer for states\n",
    "    state_input = layers.Input(shape=(state_dim,))\n",
    "\n",
    "    # Define two hidden layers with tanh activation\n",
    "    hidden1 = layers.Dense(64, activation='tanh')(state_input)\n",
    "    hidden2 = layers.Dense(64, activation='tanh')(hidden1)\n",
    "\n",
    "    # Define the output layer for the action distribution's mean\n",
    "    mu_output = layers.Dense(action_dim, activation=None)(hidden2)\n",
    "\n",
    "    # Initialize a trainable log standard deviation, independent of the state\n",
    "    log_std = tf.Variable(initial_value=np.full((action_dim,), log_std_initial_value), dtype=tf.float32, trainable=True)\n",
    "\n",
    "    # Output log standard deviation directly, using a Lambda layer\n",
    "    log_std_output = layers.Lambda(lambda x: tf.expand_dims(log_std, axis=0))(state_input)\n",
    "\n",
    "    # Construct and return the actor model\n",
    "    model = models.Model(inputs=state_input, outputs=[mu_output, log_std_output])\n",
    "    return model\n",
    "\n",
    "def critic_creator(state_dim):\n",
    "    \"\"\"\n",
    "    Creates a critic model for estimating state values.\n",
    "\n",
    "    Parameters:\n",
    "    - state_dim (int): Dimension of the input state.\n",
    "\n",
    "    Returns:\n",
    "    - A Keras model that outputs the value of given states.\n",
    "    \"\"\"\n",
    "    # Define the input layer for states\n",
    "    state_input = layers.Input(shape=(state_dim,))\n",
    "\n",
    "    # Define hidden layers and the output layer for state value estimation\n",
    "    hidden1 = layers.Dense(64, activation='tanh')(state_input)\n",
    "    hidden2 = layers.Dense(64, activation='tanh')(hidden1)\n",
    "    value_output = layers.Dense(1, activation=None)(hidden2)\n",
    "\n",
    "    # Construct and return the critic model\n",
    "    model = models.Model(inputs=state_input, outputs=value_output)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7bHqxD-lwyr"
   },
   "source": [
    "### Sample Trajectory\n",
    "Implement sample_traj function to sample trajectories from the environment.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aawoMYcIlxNx"
   },
   "outputs": [],
   "source": [
    "def sample_traj(batch_size=2000, seed=None):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to samples trajectories from the environment using the provided actor model.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size (int): The number of states visited.\n",
    "    - seed (int, optional): Seed for the environment's random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - States, actions, rewards, not_dones, and average episodic reward.\n",
    "    \"\"\"\n",
    "    states, actions, rewards, not_dones = [], [], [], []\n",
    "    curr_reward_list = []\n",
    "\n",
    "    # Continue sampling until reaching the specified batch size\n",
    "    while len(states) < batch_size:\n",
    "        # Reset the environment at the start or after each episode ends\n",
    "        state = env.reset(seed=seed)[0]\n",
    "        curr_reward = 0\n",
    "        done = False\n",
    "        #print(\"state\", state)\n",
    "\n",
    "        # Sample actions from the actor and step through the environment until the episode ends\n",
    "        while not done:\n",
    "            # Prepare the current state for the actor model\n",
    "            state_tensor = np.expand_dims(np.array(state, dtype=np.float32), axis=0)\n",
    "\n",
    "            # Get the mean and log standard deviation of action distribution\n",
    "            mean, log_std = actor(state_tensor)\n",
    "\n",
    "            # Calculate the standard deviation\n",
    "            std = tf.math.exp(log_std)\n",
    "\n",
    "            # Sample an action from the Gaussian distribution\n",
    "            action = mean + (tf.random.normal(shape = mean.shape) * std)\n",
    "\n",
    "            # print(\"debug1\")\n",
    "            # Execute the action in the environment to get the next state and reward\n",
    "            next_state, reward, terminated, truncated,_ = env.step(action)\n",
    "            reward = reward[0].astype(np.float32)\n",
    "            # print(\"next_state\", type(next_state), next_state)\n",
    "            \n",
    "            # print(\"reward\", type(reward), reward)\n",
    "            # print(\"terminated\", type(terminated), terminated)\n",
    "            # print(\"truncated\", type(truncated), truncated)\n",
    "\n",
    "            # Check if the episode has ended\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store the current state, action, reward, and continuation flag\n",
    "            states.append(state.flatten())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            not_dones.append( not done) # flip the boolean? \n",
    "\n",
    "            # Prepare for the next step\n",
    "            state = next_state\n",
    "            curr_reward += reward\n",
    "\n",
    "            # Exit the loop if the episode has ended\n",
    "            if done:\n",
    "\n",
    "                break\n",
    "\n",
    "        # Keep track of the cumulative reward for this episode\n",
    "        curr_reward_list.append(curr_reward)\n",
    "\n",
    "    # Return the sampled data and the average reward per episode\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(not_dones), np.mean(curr_reward_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHaI6w4BSc3Q"
   },
   "source": [
    "### Training Function\n",
    "As we saw in the class, the objective in Policy Gradient is to find policy parameters $\\theta$ that maximize the policy value (expected return), which is formalized as(for $\\gamma=1$):\n",
    "\\begin{equation}\n",
    "\\arg \\max _\\theta V(\\theta) = \\arg \\max _\\theta \\sum_\\tau P(\\tau ; \\theta) R(\\tau),\n",
    "\\end{equation}\n",
    "where $\\tau=\\left(s_0, a_0, r_0, \\ldots, s_{T-1}, a_{T-1}, r_{T-1}\\right)$ is a state-action trajectory,\n",
    "$P(\\tau ; \\theta)$ is probability of observing trajectory $\\tau$ when using policy $\\pi_\\theta$ starting from state $s_0$ and\n",
    "$R(\\tau)=\\sum_t R\\left(s_t, a_t\\right)$ is the sum of rewards in trajectory $\\tau$.\n",
    "\n",
    "In the lecture, the gradient of the expected return, approximated with empirical estimates from $m$ trajectories under policy $\\pi_\\theta$, and leveraging temporal structure, is given by\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)] \\approx \\frac{1}{m} \\sum_{i=1}^m \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta\\left(a^{(i)}_t \\mid s^{(i)}_t\\right) G_t^{(i)},\n",
    "\\end{equation}\n",
    "where $G_t^{(i)}$ is the reward-to-go (the sum of rewards after a point in a trajectory) and $G_t^{(i)}=\\sum_{t^{\\prime}=t}^{T-1} r_{t^{\\prime}}^{(i)}$.\n",
    "\n",
    "To mitigate the variance of the gradient estimates, we introduce a critic network, which fits a baseline function $b\\left(s^{(i)}_t\\right)$ to the reward-to-go $G_t^{(i)}$, by minimizing the loss function:\n",
    "\\begin{equation}\n",
    "\\mathcal{L}=\\frac{1}{mT} \\sum_{i=1}^m \\sum_{t=0}^{T-1} \\left\\|b\\left(s^{(i)}_t\\right)-G_t^{(i)}\\right\\|^2.\n",
    "\\end{equation}\n",
    "\n",
    "For a paritcular trajectory $i$, the advantage estimate $\\hat{A}_t^{(i)}$ is formally defined as the difference of $G_t^{(i)}$ from the baseline value of $s_t$ :\n",
    "\\begin{equation}\n",
    "\\hat{A}_t^{(i)}=G^{(i)}_t-b\\left(s^{(i)}_t\\right).\n",
    "\\end{equation}\n",
    "Incorporating the advantage estimate, the gradient estimate based on $m$ trajectories is refined to:\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[R(\\tau)]\\approx \\frac{1}{m} \\sum_{i=1}^m \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta\\left(a^{(i)}_t \\mid s^{(i)}_t\\right) \\hat{A}_t^{(i)}.\n",
    "\\end{equation}\n",
    "This gradient is then utilized in stochastic gradient ascent, iteratively adjusting $\\theta$ to improve the policy based on the accumulated experiences from the environment.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wyqgh_wzgjEj"
   },
   "outputs": [],
   "source": [
    " def train(states, actions, rewards, n_dones):\n",
    "    \"\"\"\n",
    "    TODO: Complete this block to update the actor and the critic model based on the collected batch of experience.\n",
    "\n",
    "    Parameters:\n",
    "    - states: Observed states from the environment.\n",
    "    - actions: Actions taken by the actor.\n",
    "    - rewards: Rewards received for taking actions.\n",
    "    - n_dones: Indicates whether the episode continues (1) or ends (0).\n",
    "    \"\"\"\n",
    "    # Convert to tensor\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.float32)\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    n_dones = np.array(n_dones, dtype=np.float32)\n",
    "\n",
    "    '''\n",
    "    TODO:\n",
    "    1. Compute the reward-to-go G_t using discount_rewards function\n",
    "    2. Compute values of states, this will be used as the baseline\n",
    "    3. Update the critic model to predict the reward-to-go G_t for each state\n",
    "    4. Compute log probabilities and advantages\n",
    "    5. Compute the loss of the actor model based on policy gradient estimate and update the actor\n",
    "    '''\n",
    "    # Calculate discounted rewards (num_traj is the number of trajectories)\n",
    "    G_t, num_traj = discount_rewards(rewards, n_dones)\n",
    "\n",
    "    # Update the critic model\n",
    "    with tf.GradientTape() as tape:\n",
    "        critics = critic(states, training=True) #their code\n",
    "        critic_loss = tf.keras.losses.MSE(critics, G_t)\n",
    "\n",
    "    critic_grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "    critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "\n",
    "    # Update the actor model\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute log probabilities\n",
    "        \n",
    "        means, log_stds = actor(states)\n",
    "        stds = tf.exp(log_stds)\n",
    "        neg_log_prob = 0.5 * tf.reduce_sum(tf.square((actions - means) / (stds + 1e-8)), axis=1)\n",
    "        neg_log_prob += 0.5 * np.log(2.0 * np.pi)\n",
    "        neg_log_prob += tf.reduce_sum(log_stds, axis=1)\n",
    "\n",
    "        # Compute and normalize the advantages tensor\n",
    "        advantages = G_t - critics\n",
    "        advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
    "\n",
    "        # Compute the loss based on policy gradient estimate\n",
    "        actor_loss = tf.reduce_sum(neg_log_prob*advantages)/num_traj\n",
    "\n",
    "    actor_grads = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "    actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "\n",
    "def discount_rewards(reward_buffer, n_dones):\n",
    "    \"\"\"\n",
    "    TODO: Complete this function to compute the reward-to-go G_t. Note that reward_buffer may include rewards of several trajectories and n_dones can be used to Indicate whether one trajectory ends.\n",
    "\n",
    "    Parameters:\n",
    "    - reward_buffer: The rewards to be processed.\n",
    "    - n_dones: Indicates whether the episode continues (1) or ends (0).\n",
    "\n",
    "    Returns:\n",
    "    - G_t (reward-to-go ), num_traj (the number of trajectories)\n",
    "    \"\"\"\n",
    "    G_t = np.zeros_like(reward_buffer, dtype=float)\n",
    "    running_add = 0\n",
    "    num_traj = 0\n",
    "    for t in reversed(range(len(reward_buffer))):\n",
    "        # Reset the accumulator and count the number of trajectories at the end of each episode\n",
    "        \n",
    "        if n_dones[t] == 0:\n",
    "            running_add = 0\n",
    "            num_traj += 1\n",
    "        running_add = running_add * GAMMA + reward_buffer[t]\n",
    "        G_t[t] = running_add\n",
    "    return G_t, num_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JA7a0AXiVO7"
   },
   "source": [
    "### Guidelines for Implementing and Optimizing a VPG Agent\n",
    "- **Network Updates**: During each training episode, we collect multiple trajectories and update networks. Each trajectory has 200 state-action-reward samples. Experiment with the batch size (the number of samples collected in one episode) to find a balance between sample efficiency and learning stability.\n",
    "\n",
    "- **Training Scale**: Typically, achieving satisfactory training results requires around 800 to 1200 episodes, with each episode handling a batch size of 5000 (equivalent to 25 trajectories). Note that this range can vary due to the randomness inherent in the Pendulum environment.\n",
    "\n",
    "- **Parameter and Model Architecture Tuning**: Experiment with the parameters and model architecture. Consider implementing a schedule for adjusting parameters such as the learning rate.\n",
    "\n",
    "- **Efficiency**: While achieving high performance with minimal episodes is ideal, focus on implementing VPG effectively. Efficiency improvements are encouraged but not required for grading.\n",
    "\n",
    "- **Debugging Tips**: Monitor the training process by logging the average reward per episode and other metrics. These indicators can help identify training stability and convergence.\n",
    "\n",
    "\n",
    "- **Visualization**: It's important to visualize the agent's performance over time by plotting the running reward during training.\n",
    "\n",
    "- **Objective**: Implement a VPG agent. Your goal is to tweak the neural network and training parameters to achieve a high reward, aiming to consistently achieve rewards above -200. Even so, the grading criteria for rewards are relatively flexible.  \n",
    "\n",
    "*Optional:* Explore advanced techniques for further improvement, such as Advantage Actor-Critic (A2C or A3C). Use generalized advantage estimates (TD with a proper look ahead) instead of using the returns, to improve efficiency\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "u11sKN5uhUXG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/opt/anaconda3/lib/python3.11/site-packages/gymnasium/utils/passive_env_checker.py:246: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Episode Reward: -1312.6112346336245, Running Reward: -1312.6112346336245, Runtime: 4.3196961879730225 seconds, Total Runtime: 4.319777011871338 seconds\n",
      "Episode: 2, Episode Reward: -1342.156088470146, Running Reward: -1327.3836615518853, Runtime: 4.166988849639893 seconds, Total Runtime: 8.48701786994934 seconds\n",
      "Episode: 3, Episode Reward: -1138.1893032203986, Running Reward: -1264.3188754413898, Runtime: 4.198083877563477 seconds, Total Runtime: 12.685178995132446 seconds\n",
      "Episode: 4, Episode Reward: -1269.337790067233, Running Reward: -1265.5736040978506, Runtime: 4.089270114898682 seconds, Total Runtime: 16.774523973464966 seconds\n",
      "Episode: 5, Episode Reward: -1439.4434559081121, Running Reward: -1300.347574459903, Runtime: 4.09662389755249 seconds, Total Runtime: 20.871217966079712 seconds\n",
      "Episode: 6, Episode Reward: -1562.9107623505965, Running Reward: -1344.1081057750187, Runtime: 4.087106943130493 seconds, Total Runtime: 24.958396911621094 seconds\n",
      "Episode: 7, Episode Reward: -1585.9570564192907, Running Reward: -1378.6579558670576, Runtime: 4.152085065841675 seconds, Total Runtime: 29.110545873641968 seconds\n",
      "Episode: 8, Episode Reward: -1622.4891417366266, Running Reward: -1409.1368541007537, Runtime: 4.0818376541137695 seconds, Total Runtime: 33.19246172904968 seconds\n",
      "Episode: 9, Episode Reward: -1626.1338697882834, Running Reward: -1433.2476336215905, Runtime: 4.094670057296753 seconds, Total Runtime: 37.28719687461853 seconds\n",
      "Episode: 10, Episode Reward: -1612.0041296136378, Running Reward: -1451.1232832207952, Runtime: 4.063634872436523 seconds, Total Runtime: 41.35089683532715 seconds\n",
      "Episode: 11, Episode Reward: -1603.8703901231288, Running Reward: -1465.00938384828, Runtime: 4.109344959259033 seconds, Total Runtime: 45.460309982299805 seconds\n",
      "Episode: 12, Episode Reward: -1610.1855964303018, Running Reward: -1477.1074015634485, Runtime: 4.120819807052612 seconds, Total Runtime: 49.58119773864746 seconds\n",
      "Episode: 13, Episode Reward: -1620.0240323147177, Running Reward: -1488.1009885443154, Runtime: 4.085522174835205 seconds, Total Runtime: 53.66678500175476 seconds\n",
      "Episode: 14, Episode Reward: -1617.296258393731, Running Reward: -1497.329222104988, Runtime: 4.084112167358398 seconds, Total Runtime: 57.750967025756836 seconds\n",
      "Episode: 15, Episode Reward: -1601.360616799593, Running Reward: -1504.2646484179616, Runtime: 4.104198217391968 seconds, Total Runtime: 61.855234146118164 seconds\n",
      "Episode: 16, Episode Reward: -1636.2257971207284, Running Reward: -1512.5122202118844, Runtime: 4.078044891357422 seconds, Total Runtime: 65.93333792686462 seconds\n",
      "Episode: 17, Episode Reward: -1630.2860880461335, Running Reward: -1519.4400947903696, Runtime: 4.082968235015869 seconds, Total Runtime: 70.01638507843018 seconds\n",
      "Episode: 18, Episode Reward: -1612.3519091294706, Running Reward: -1524.601862253653, Runtime: 4.08188796043396 seconds, Total Runtime: 74.0983407497406 seconds\n",
      "Episode: 19, Episode Reward: -1605.6006234199554, Running Reward: -1528.8649549466163, Runtime: 4.122270107269287 seconds, Total Runtime: 78.22067999839783 seconds\n",
      "Episode: 20, Episode Reward: -1621.1203551123106, Running Reward: -1533.477724954901, Runtime: 4.0777599811553955 seconds, Total Runtime: 82.29851293563843 seconds\n",
      "Episode: 21, Episode Reward: -1615.0471291085332, Running Reward: -1537.3619822955502, Runtime: 4.158123970031738 seconds, Total Runtime: 86.45670795440674 seconds\n",
      "Episode: 22, Episode Reward: -1627.9888122377172, Running Reward: -1541.4813836565577, Runtime: 4.086136817932129 seconds, Total Runtime: 90.54291582107544 seconds\n",
      "Episode: 23, Episode Reward: -1597.84423584342, Running Reward: -1543.9319424472908, Runtime: 4.139772891998291 seconds, Total Runtime: 94.68276000022888 seconds\n",
      "Episode: 24, Episode Reward: -1610.9192464960367, Running Reward: -1546.7230801159885, Runtime: 4.094222068786621 seconds, Total Runtime: 98.7770528793335 seconds\n",
      "Episode: 25, Episode Reward: -1613.4683278462292, Running Reward: -1549.3928900251983, Runtime: 4.113503932952881 seconds, Total Runtime: 102.89062404632568 seconds\n",
      "Episode: 26, Episode Reward: -1622.0416813080012, Running Reward: -1552.1870743053062, Runtime: 4.1270997524261475 seconds, Total Runtime: 107.01778984069824 seconds\n",
      "Episode: 27, Episode Reward: -1622.6537037356943, Running Reward: -1554.7969494693946, Runtime: 4.148452281951904 seconds, Total Runtime: 111.16634607315063 seconds\n",
      "Episode: 28, Episode Reward: -1618.4720398498328, Running Reward: -1557.0710598401245, Runtime: 4.119430065155029 seconds, Total Runtime: 115.28584384918213 seconds\n",
      "Episode: 29, Episode Reward: -1629.4639413872362, Running Reward: -1559.5673661003698, Runtime: 4.1835949420928955 seconds, Total Runtime: 119.46950387954712 seconds\n",
      "Episode: 30, Episode Reward: -1599.6029236491024, Running Reward: -1560.9018846853276, Runtime: 4.099743843078613 seconds, Total Runtime: 123.56931400299072 seconds\n",
      "Episode: 31, Episode Reward: -1606.420540797338, Running Reward: -1562.370228430876, Runtime: 4.145475149154663 seconds, Total Runtime: 127.71485900878906 seconds\n",
      "Episode: 32, Episode Reward: -1613.1863381398468, Running Reward: -1563.9582318592816, Runtime: 4.087759017944336 seconds, Total Runtime: 131.8026819229126 seconds\n",
      "Episode: 33, Episode Reward: -1618.3376583567263, Running Reward: -1565.6060932682951, Runtime: 4.096609115600586 seconds, Total Runtime: 135.89936208724976 seconds\n",
      "Episode: 34, Episode Reward: -1618.2243066526948, Running Reward: -1567.153687779601, Runtime: 4.105956077575684 seconds, Total Runtime: 140.00539994239807 seconds\n",
      "Episode: 35, Episode Reward: -1614.076804227326, Running Reward: -1568.494348249536, Runtime: 4.135376214981079 seconds, Total Runtime: 144.14085507392883 seconds\n",
      "Episode: 36, Episode Reward: -1623.3449335062876, Running Reward: -1570.0179756177793, Runtime: 4.126429796218872 seconds, Total Runtime: 148.2673499584198 seconds\n",
      "Episode: 37, Episode Reward: -1609.0592627899348, Running Reward: -1571.0731455413509, Runtime: 4.100966930389404 seconds, Total Runtime: 152.36839890480042 seconds\n",
      "Episode: 38, Episode Reward: -1612.3778233404457, Running Reward: -1572.1601107465901, Runtime: 4.176086187362671 seconds, Total Runtime: 156.54455304145813 seconds\n",
      "Episode: 39, Episode Reward: -1594.2622612544149, Running Reward: -1572.726832554483, Runtime: 4.080387115478516 seconds, Total Runtime: 160.62501192092896 seconds\n",
      "Episode: 40, Episode Reward: -1598.1316388767957, Running Reward: -1573.3619527125406, Runtime: 4.09964394569397 seconds, Total Runtime: 164.72472882270813 seconds\n",
      "Episode: 41, Episode Reward: -1582.496721045226, Running Reward: -1573.584751940167, Runtime: 4.085152864456177 seconds, Total Runtime: 168.8099558353424 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ns/hfpwj42x3zd2qyh387_9zcvm0000gn/T/ipykernel_70038/3104383348.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Start time of the current episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Sample trajectories using the current policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_dones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_traj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Update the actor and critic models using the sampled trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_dones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ns/hfpwj42x3zd2qyh387_9zcvm0000gn/T/ipykernel_70038/4087483257.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(batch_size, seed)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Calculate the standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Sample an action from the Gaussian distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# print(\"debug1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Execute the action in the environment to get the next state and reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# and the tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_add_dispatch_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m         _ctx, \"AddV2\", name, x, y)\n\u001b[1;32m    479\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       return add_v2_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO: Implement the training loop to sample trajectories and update the actor and critic models accordingly.\n",
    "\"\"\"\n",
    "# Parameters for training\n",
    "GAMMA = 0.99  # Discount factor for expected discounted sum of rewards\n",
    "last_n_reward = 100  # Number of episodes to consider for calculating running reward\n",
    "\n",
    "# Feel free to change parameters below\n",
    "TRAIN_EPISODES = 1500  # Total number of episodes for training\n",
    "actor_lr = 3e-3  # 4e-3 Learning rate for the actor optimizer\n",
    "critic_lr = 3e-3  # 2e-3 Learning rate for the critic optimizer\n",
    "batch_size = 5000  # Number of steps per batch, 5000\n",
    "\n",
    "# Initialization\n",
    "actor = actor_creator(obssize, actsize)  # Create the actor model\n",
    "critic = critic_creator(obssize)  # Create the critic model\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=actor_lr)  # Actor optimizer\n",
    "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=critic_lr)  # Critic optimizer\n",
    "episode_reward_history = []  # Stores the reward of each episode\n",
    "running_rewards = []  # Stores the running rewards\n",
    "initial_time = time.time()  # Start time for measuring training duration\n",
    "\n",
    "# Training loop\n",
    "for episode in range(TRAIN_EPISODES):\n",
    "    start_time = time.time()  # Start time of the current episode\n",
    "\n",
    "    \n",
    "    # Sample trajectories using the current policy\n",
    "    states, actions, rewards, not_dones, episode_reward = sample_traj(batch_size = batch_size)\n",
    "\n",
    "    # Update the actor and critic models using the sampled trajectories\n",
    "    train(states, actions, rewards, not_dones)\n",
    "\n",
    "    # Save the reward\n",
    "    episode_reward_history.append(episode_reward)\n",
    "\n",
    "    # Keep only the last n rewards\n",
    "    if len(episode_reward_history) > last_n_reward:\n",
    "        del episode_reward_history[0]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    running_rewards.append(running_reward)\n",
    "\n",
    "    # Track the running time\n",
    "    end_time = time.time()\n",
    "    episode_runtime = end_time - start_time\n",
    "    total_runtime = end_time - initial_time\n",
    "\n",
    "    # Print training information\n",
    "    print(f\"Episode: {episode + 1}, Episode Reward: {episode_reward}, Running Reward: {running_reward}, Runtime: {episode_runtime} seconds, Total Runtime: {total_runtime} seconds\")\n",
    "\n",
    "    # Save the actor model if the condition is met\n",
    "    if episode_reward >= -200:\n",
    "        consistency_count += 1\n",
    "        if consistency_count == 1: #arbitrary \n",
    "            actor.save_weights(\"best_actor_weights.h5\")\n",
    "            print(\"Model saved.\")\n",
    "            break\n",
    "    else:\n",
    "        consistency_count = 0\n",
    "\n",
    "# Plotting the running rewards to visualize training progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(running_rewards)\n",
    "plt.title('Running Rewards Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Running Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uguKEECCt-8M"
   },
   "source": [
    "### Testing\n",
    "Evaluate the performance of the agent on 100 episodes on the environment and print out the average testing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSY34gwLpgYZ"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE Here\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1aCi4D6WLSJVU2gP9qPxzhIR3pHHGyzem",
     "timestamp": 1712955139312
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
